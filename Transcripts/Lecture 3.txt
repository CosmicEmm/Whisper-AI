 Assalamu alaikum wa rahmatullahi wa barakatuh. In the name of Allah, the most gracious, the most merciful. Insha'Allah today we will talk about supervised learning in more detail. Last time we went through very quick definitions, our introduction with examples, to the three types of supervised learning. What were these three types? Remember? What are the three types we discussed last time? Supervised, unsupervised and reinforcement learning. Quickly, what is the difference between supervised and unsupervised learning? Yeah, the labeled data, right? Which one has labeled data? The supervised learning. So supervised learning, we are given labeled data to guide the learning, but in unsupervised we don't. Today, insha'Allah, we will talk in a bit more detail about supervised learning. We will go through, first we'll go through an overview with giving you some notations that we will use throughout the semester. And then we will talk about very simple example of supervised learning, but it is really good example because we, using it, we will go through all the main concepts of supervised learning and even in general machine learning. So we start with an overview of supervised learning. Last time we said that supervised learning is about mapping from one or more inputs to one or more outputs, in general. And by mapping we mean a mathematical equation. So mathematical equation that will take the value of the input and will give us the value of output. So the model is just an equation. This equation we can later use to compute predicted output. So if I give you, it will be an equation of the input values, right? I will give it the value of the input and then I will get back a value for the output. We call this inference. So for example, if you remember last time we talked about kind of prediction of estimated cost of a house. Today we'll talk about an example of estimating price of a car. Given two input values, the age of the car and the mileage of the car, and we want to estimate the price of the car. If I have a model that represents the mapping between the input and the output, I can give it a specific value of the age and specific value of the mileage. And then from that equation we can get a value for the estimated price of the car. We call this again inference. Now in general, a model, as we said, it's a mathematical equation, right? The equation in general has parameters. Has parameters. When we change these parameters, we actually change the equation, which means we are changing the output of that equation. We'll talk about an example to make it very clear, inshallah, later today. But in general, as I said, an equation, any mathematical equation, has parameters. If we know the values of the parameters, we can compute the value of the function for any input value. And that's why we say that a model doesn't just represent one equation, but represents a family of equations. Why is it a family of equations? Because we have parameters. With every specific set of values for the parameters, we get one specific equation. Okay? If we change the values of the parameters, we change the equation, which means we change the model. So in general, the model represents a family of equations. If we are given specific data, then we can find specific values of the parameters for a specific model that might be good for such data. And we'll see that, inshallah, today. And that's exactly what we mean by training a model. When we train a model, that means that we want to find the parameters that will predict good values for this data. That will give us good predictions. Okay? For this set of data. If we change the values of the parameters, of course the predicted values will be different. So we are trying in training, we are trying to find a good model, a good set of values of the parameters that will fit well to the training data. Now let's talk about the notation that we will use, inshallah, in this semester. And it will be matching what the textbook is using. We have input. Let me change this pointer. We have the input. And the input can be normal, which means that it's not bold, it's not italic. It means that it will be scalar, just one value. And the output is the same. If it's bold, then it's a vector of values. Okay? If it's capital and bold, that means that it's a matrix. So that's the notation of the input and the output. And about the model, again, the model is a function. Okay? And a function of the input. If I give the model the input values, then I would get back the output value. The model, the name of the model, usually we use the F, G, and so on as names of the functions of the models. Similarly, normal means it returns scalar. Bold means it returns a vector of values. And capital means it returns a matrix. Here's the example. X here, it's bold. It represents a vector of values, which are the age and mileage. And here the output is just one output, one value, which is the predicted price. Okay? So that's why it's a scalar. It's just one value. It's not bold here. And the function, again, it represents one just output. So it's not bold. Okay? Any question about this notation? Yes? Yes. No, no, no. It doesn't matter. It doesn't. It's not. No, no, no. It's not always, of course. Okay? The input can be multiple. The output might be one. Anything. It can be many, many too many in general. Okay? Yes. Yeah, X here is bold, which means that it is a vector. So that's the notation. I'm just getting the quick understanding of how to represent these values. When we have more than one value, when we have vectors of values, we call this structured or tabular data because we can put this data in a table. Where the columns are the values and the rows are the examples. Okay? It's just will be easier to think of it as a table. Now, the model, we said that the model has parameters, right? We usually use the Greek letters for the parameters. Here we use phi. Later we use theta and phi and so on. And the model in reality is a function of both the input and the parameters. So usually we have f of X and phi. And in short, we just use f of phi. We just ignore X. That doesn't mean that the input is not given to the function, but we just say that, we just think that this is implicit. Okay? And we will always think of the model as a function of the parameters. So X is the input. Okay? And phi are the parameters of the model. We will see examples in sha Allah this lecture. No, no, no. X is the input values. Let's get back here. X is the real input values, like for a specific car, the age of the car and the mileage of the car. Okay? The parameters phi are the parameters of the model itself. If we will represent, as we will see later in sha Allah today, if we will represent the relationship between the age and the mileage and the predicted price as a line, then phi will be the parameters of the line. The intercept and the slope of the line. We'll see that. Okay? Okay now. Now, in order to measure how good the model is when we do the training, we will use what we call the loss function. We also call it cost function or error function. So you can think of it as measuring how far we are, how far the predicted values of the model from the actual values that we should get. Okay? So it should give us an indication of how good the model is. If the loss value is very high, what does that mean? We are losing much, right? Which means this is good or bad? Bad, of course. Okay? If the loss value is very small, that means that we are closer to the actual values, the output values that we want. So we compute the loss function when we do training. We compute the loss function on or using the training examples. Remember, in supervised learning, the training examples are labeled, right? We have input and corresponding output. Input and corresponding output. So we know the corresponding output is the actual output that we want to model. Okay? So given the training data, we use the notation X, I, and Y, I. So this is one example, one pair of input and output. And we have I pairs of them in the training data. Okay? From one to I. Now, given this training data, we can define a loss function. So the loss function is a function of the parameters of the model. The model itself and the training data. Let's explain that. We want to know or to measure how good the model is, right? I think we agree that we need the training data because we need to compare between the predicted values and the actual output values. Right? So the training data has to be part of the loss function. Okay? We are also computing the loss function for a specific model. Right? We want to see if that model is good or bad. Right? So if we want, if we are talking about specific model, then we are talking about specific values of the parameters. That's why the parameters are input for that loss function. So we have to know the parameters. Fine. The parameters of the model. Okay? We have to know the parameters of the model because we are talking about the loss value or how good a specific model is. And we have to have the model equation. Right? Why do you need the model equation? We said that we want to compare between the predicted value and the actual values. The actual values are in the training data. Right? Where is the predicted value? Okay? So we have to compute to get it from the equation. Okay? Which is the model function. The model equation. So we need to know the parameters. We need to know the model. And we need to know how to get the training data. In short, we just use the parameters. We just say that the loss function is a function of the parameters. Of course, it's also a function of the model and the function of the training set. But for short, for simplicity, we just say that the loss function is a function of the parameters. Which means it's a function of the model. Right? Because if I gave you the specific values of the parameters, then that means that I'm pointing to a specific model. Okay? All of that, inshallah, will be more clear with the regression example that we will discuss today. Now, in training, we can use the loss function to see how good the model that we have now is. But our target is to get the best possible model. Right? Which means that we need, in training, to find the values of the parameters that will make the loss the minimum possible. Right? We said that the loss is like the error. Right? So we want to minimize the error. So in training, we want to get the minimum, we want to get the values of the parameters that will make the loss function the minimum. Are you familiar with this argmin notation? Terminology? So argmin means, yeah, find, this will return the values of that variable that will make this mathematical function the minimum. Okay? So that will return what we call phi hat, which is the values of the parameters that will give us the best model. So this is actually the best model we can get because it minimizes the loss function. The best model that we can get from the training data that we argue. Yes. Is a function that will return the values of the parameters that will minimize this function. In general, argmin can be used with any function, with any variable. Okay? So it gives us the minimum. This is just a notation, by the way. How to implement this function? This is bad. I mean, it would be a lecture or two. Okay? But this is just a notation that we target in the training phase, we target or the goal is to find the parameters, phi hat, that will minimize this loss function. Why do we need to minimize the loss function? Yeah, because the loss function represents an error, right? And we want to minimize the error between what we predict from the model and the actual output of the, in the training data. Any questions so far? Okay. Now, after we train a model, what do we have? When we say we trained a model, what does that mean? Back to the previous slide, what does that mean? We found specific values for the parameters that hopefully will minimize the loss function. Okay? But we have a specific model now. Right? Type. Is it guaranteed that this is really the best model? We have to test it, right? After we trained it, we have to test it. Type. To test it, we have to do the testing on a set of examples that was not used in training. Otherwise, it would be like cheating, right? Okay. So, for example, the classic example that I usually use is the studying a course. Okay? When you study a course, you are trained using, with solving problems. The instructor tries to solve problems with you, right? Okay? So you are trained with homework, with project, with quizzes. Right? But then, in the, after you solve many problems, you will be tested on problems that you have seen already in the, in the, in the, no. Okay? Usually they are similar but not exactly the same. Right? Otherwise, you would memorize the answers, right? And that will not be a good indication of how good you are in understanding the material of the course. So the test set has to be separate from the training set. Separate means different. Type. In teaching you deep learning, should I test you on chemistry problems? It should be also in deep learning, right? But in different problems. But in the same field. Same distribution. Why we need to test on a separate set? So you mean the performance might not be good. We need to see the actual performance. Is it, is our goal in reality to test on just any unseen, any separate set? Or to, to see, to get an indication of how good the model is when it is tested on unseen data in general? In reality. I mean, this model should be used in, in the real, in the real world, right? It will see examples that it saw in training in the real world. No, it will see unseen data before. Right? So this is what we call generalization. We want to get an indication of how general the model is. How the model generalizes. Generalizes to what? To unseen data. To data that was never seen to it before. And using the test set is a way to get that indication. Okay? Because the test set, we will try to think of the test set as if it is data in the real world. That, that the model didn't see before. Okay? That's why it has to be separate from the, from the training set. So, this will make us see how well it, it can generalize to new data. New data means data that is never seen by the model. Okay? Is that clear? So I always do that. The, the rule of thumb here is never ever touch your test data. What does that mean? When you train a model, you should not look. Touch means, and it doesn't mean touch physically. But it means that you should not look at all to the test data. The model doesn't look at the test data or you doesn't look at the test data. Both. Both. Okay? It's clear why the model should not see the test data, right? Otherwise it will consider it in training. But why don't you look at the data? Not the model. Because you will be biased, right? If you look at the test data, you'll see, okay, this special case is, let me cover it in the model. Okay? Let me change the model so that it will perform well on these cases. Even if it's unintentional. Okay? You never know how, how we can consider it. So the test data should be completely separate. Okay? Any questions on that? Let's take a quick poll. Supervised learning is mapping, mapping from one input to one output. Mapping from one input to multiple outputs. Yeah? Mapping from multiple inputs to one output. Which one? Yes? So you mean it should be mapping from? Exactly. So it's none of them actually. Okay? So none of them. It should be mapping from one or more inputs to one or more outputs in general. Okay? Okay? A learning model has training data, test data, parameters, all of them. Think, think before you answer, please. A learning model, the model itself has training data, has test data, has parameters, has all of these three things. Why should we say that the learning model has training data and test data? The model has the data? No. Now we use the data, we use the training data to train the model. We use the test data to measure how good the model generalizes. But that's not part of the model. The model has parameters. Okay? The data is not part of the model. The data is completely separate from the model. It's different. Okay? We use the data to fit the model to, for a specific problem. Okay? Is that clear? Please think before you answer. Okay. Generalization of a model, generalization of a model is about how well it performs on the training set, how well it performs on the test set, how well it performs on both, or how well it performs on unseen data. Who says the first one? Second one? Third one? Fourth? Okay. Yeah. So it performs on unseen data. So why don't we say performance on the test set? Huh? Yeah. So it should be also on the test set, right? The test set is unseen. But not only on this, we want to know what it performs on any other test set. Yeah. So it's not only, our goal is not only on the test set. We use the test set as a means to measure how it generalizes. But our goal, the concept of generalization, is not about specifically the test set, because we can't change the test set by the way. The test set is just a subset from reality. Okay? But the goal in general is to perform on unseen data. Okay. Now let's talk about, after getting this quick introduction, overview, let's talk about very specific example of supervised learning. Although this is probably the simplest example we can discuss, but it will show us exactly, I hope, how to understand the concepts that we just mentioned. So the example will be 1D linear regression. What do we mean by regression? I think we discussed that before. What is regression? Yes? Yeah. When the output is? Is continuous value. Okay? When the output is continuous. When we say linear regression, that means that the model will be linear. It will be a linear equation, as you will see. And 1D means the input is just one input. So it's the simplest ever. So it predicts a single output from a single input. And to do that, it describes the relationship between the input and the output as a straight line. As a straight line. So the model is, as I said, is just an equation, a linear equation. And an equation of a line, how many parameters we have? Just two, right? The intercept with the y-axis and the slope. So the parameters are two, phi 0, which is the y-offset or y-intercept, and phi 1, which is the slope. And here is one example line. And with this line, the phi 0 is 0, so the intercept with the y-axis is 0. And phi 1 is 1, which means that the slope is 1. Which has 45 degrees. This function is a function of one line? Of one specific line? No, it's a function of many lines. How many lines we can represent with that function? Infinite number of lines, right? But if I know the specific values of phi 0 and phi 1, then we are presenting a specific line, like this case. If I change phi 0 and phi 1, I will get a different line. And remember here, the line is the relationship that we want to get from the input to the output. Because here is the input, here is the output. It's a single input, single output. So that line represents the relation between x and y. If we change the values of the parameters, we get a different line. So the green line here is a different line. Phi 0 is 1.2, so the intercept is 1.2 with the y-axis. And the slope is minus 0.1. Okay, here's the third one. When phi 0 is 1, the intercept is at 1, and the slope is negative, 0.4. And we can have so many lines. So with every different set of values or parameters, we get another line. So as we said, this equation represents a family of possible input-output relations. It represents actually all possible lines. If we want to point to a specific line, then we are talking about specific values of... Phi 0 and phi 1. Very good. Which line we will get? Which model we are targeting? Which equation we want? It depends on... This error on what? On the training data. So it depends on the training data. If you give me training data, I can fit a specific line to them. If you change the training data, that might be a different line. But all of them are lines, are linear regression models. So you understand that we have so many lines represented by this equation. Which line we will use as a model? It depends on the problem that we are solving. In the problem we are solving, we are given specific data, specific label data. Input and output pairs. Remember, this is supervised learning. So we have the input and output, input and output. So many pairs of input and output. So we can now, given this data, we can try to find a line that is good for them. Good to match them. Good question. In general, it doesn't have to be a line, of course. But here we are talking about linear regression. So we are trying to fit a line to any data that will be given. Even if the data is not actually, or should not be represented as a line. Okay? That's a very good question. Now, let's say that we want to get a specific line. Right? How can we find a good one? We need an indication, sorry? You have another question? Sure. Yes, how can we find such line? Yeah, we need to have an indication of how good the line is. Right? And to do that we use what we call the loss function. Okay? So as we said earlier, the loss function is an indication of the error. Right? So we have to have the loss function, a specific loss function. And we use it to guide the choice of the line. Okay? Usually for the regression problems, we use a loss function called least squares loss function. What is it? Now we want to compare between the predicted values and the actual values. So we have a loss function, and we have a loss function. So we have a loss function, and we have a loss function. We want to compare between the predicted values and the actual values. Right? What are the predicted values? Given a specific model, we give the value of X, and we compute the output of the linear function. Right? That will be the predicted value. We want to compare it with the actual value of that input. That's exactly what we do here. So f of X, I, and phi, what is phi? The parameters, the vector of the parameters. It's not just one, by the way, right? It is bold. It's a vector. So f of X, I, and phi is the predicted value given X, I. The predicted value for X, I. Okay? Where is the actual value of X, I? Y, I. So we subtract the predicted value minus the actual value. And because we don't care of the direction, we don't care of the sign, we don't care whether one is higher than the other or not, we care about the difference between them, so we square it to get rid of the sign. So we square it. This is for just one example, right? So we have to sum over all what? No, not all the parameters. All... What are these? What do we call these points? No, not input. Training data. All the training examples. Okay, remember we have I training examples. So over all training examples, for each one, we will compute the predicted value minus the actual value for it. This is like an error for one example, right? Square it. That's why it's least squares. And sum over all training data. That will give us one value, right? So that will give us one value, which is the loss for specific model, where is the specific model? How do we know that it's a specific model? Yeah, because we are using specific parameters here. Phi is specific parameters indicating one model. Do you have a question? No, the actual values are given, of course. Remember supervised learning? We have labeled data. X, Y. This is for training only. Okay, for test, we should not look at Y, of course. We don't use the values of the labels in the test set. We just compare between them. But for training, you'll see that we use them to guide the choice of the parameters. We'll talk about later, inshallah, the details. Yes. Yes, exactly. Yeah, XI is the input. No, YI is not the predicted value. It's the actual value from the training data. Let me show you an example. Here's an example. Here's a model. This is a model, right? This is a linear model. That's a line. It has the two parameters, specific values of the two parameters. Phi 0 and phi 1. So that's a specific model. How good that model is, we have to use the loss function. The loss function computes the difference between the predicted value and the actual value. Let's see how this is on that figure. Let's say that we have this point. You see this point? What is the X value for it? Let's say, for example, this is 0.45. Where is the predicted value for that input? We have to get it from the line. The predicted value is the Y value for this X, which is here. It's again like 0.45 maybe. What is the actual output value? It's 1. It's 1. Exactly. So the dotted line here is the difference. The dotted line here is the difference for that point. For every point, you see there's a dotted line. That indicates the difference between the actual value from the training data and the predicted value. We square each of these. Does that mean the model is bad? It seems like from what we see, it is bad because it's very far from the actual values. If you compute the loss in this case, it's 7.11, given the exact values of the training data. Here's another one. Again, you will compute the differences, square them, and sum them. Here you can see that some of them are positive, some of them are negative. We don't care about positive or negative. We care about how far they are. So we square them and sum them. Is that better or worse than the previous one? Why? How did you know that it's worse? Is that subjective or you can say from the loss? The loss here is 10.22, and the other one was 7.11. Based on the loss function, this one is worse or better? It's worse. Here's the third one. That's clearly better, right? Just from looking at it, and also from the loss, of course, you see that the errors are very small. Maybe there's more better than that, by the way. But this is at least better than the previous two. The loss here is 0.19. Yes? Okay, so, yeah, good question. In general, when we train, the goal is to find the global minimum of the function. Most of the time, it's not easy to find that global. So it's not easy to say, okay, this is enough, so there are some heuristics that we will use to stop the search process. We'll talk about that later in shop. Yes? Okay, this is just an example. I told you from the beginning, this is the simplest example ever. Linear regression. But it doesn't mean that always the relationships will be linear. Of course not. Life is not linear. Life is very nonlinear. Okay? But sometimes, by the way, linear regression might be good enough. Sometimes not. But it's a good enough example for us to talk about these issues. Otherwise, it will be very complex. But if you understand the concepts from that simple example, simple model, then, inshallah, the example can be easily understood for the other models that are much more complex. Okay, now let me show you how we can look at the loss function. Remember here that we tried multiple lines, right? Some of them have very large loss. Some of them have very small loss. What is our target? Our target is to find the minimum loss. So imagine that we can visualize the loss function. In that specific example, we have two parameters, right? It's a function of two parameters. What are the two parameters? Phi 0 and phi 1, the intercept and the slope, right? So we can represent phi 0 on the x-axis, phi 1 on the y-axis, and then the value of the loss itself will be the z-axis, the height. It's a 3D function. 3D because we have two parameters and then the value of the loss is the height. Okay? Of course, it's like a surface now, right? It's a 3D function. It's a surface. Not very clear because we don't see it 3D. We see it actually on 2D, so we don't have the depth here. But you can, to get an indication of the values here, you can think of the shading to indicate the values. So brightness here is high values, is indicating high values. And darkness indicating low values. Yes, so we actually target to find the darkest point of this surface. Now, what are these points? You see these three points? Each point here in that space is represented by a specific value for phi 0 and specific value for phi 1. And the height is the loss for which are specific line, right? Remember, the line, the model here is linear regression, right? It's a line. If you give me the values of phi 0 and phi 1, then we have specified a specific line. So every point here actually represents the loss for a specific line in the space. Because every point has phi 0 value and phi 1 value. And the height is the loss. So actually, this point is that line. Do you see the mapping? And this line has a higher loss, right? It's that point. In that space. So how come that a line is a point? Is that clear to you? Yeah, the line in 3D is here because it's represented by just the value of phi 0 and phi 1. And the height in that 3D space is the loss. Here, the loss value is not seen, right? It's just written, but it's not seen. And the third one, which looks the best, is this one. The lowest among the three. You don't know if it's the global lowest, the global minimum, or not. But at least the lowest. And you can see that it's the darkest, probably, on that figure. Is that figure clear? Any question on that? Okay. Right. We can also represent the loss function in a different way. Now we are presented in 2D, not 3D. And again, we use the shading to indicate the value of the loss. So brightness means higher values. So here we have phi 0 on the x-axis, phi 1 on the y-axis. We don't have z, of course, because this is 2D. But the color indicates the value of the loss. Forget about these curves now. Forget about these curves. Look at the shading and the color. In this area, that means that the loss is very high. In this area, it seems much lower. In this area, a little bit higher than this. So it seems like it's the surface. And as you see here, the surface is like that. Yeah, it's curved like that. Which is the simplest or the one that we always hope, because that's the simplest to find the minimum. But again, life is not like that. But here in this case, it's very, very simple. We call it a convex function. In the convex function, we can find the global minimum easily, at least much easier than in other cases. And again, the shading here, the darkness means we are going towards the minimum of the function. So what are these three points? The same three points here. But now they are represented in this 2D space. And you can see again and again, this one is in the darkest area. Relative to the other two. Okay. Now, what are these curves? We call these contours. Each one of these represent a level of the loss function. Every point, and let's say that we are talking about this curve, this contour, every point on that contour has the same equal loss value. So all points on that contour have the same loss value. All points on that other contour have the same loss value. And so on. Okay. So it indicates to you where are the points that have the same loss value. So according to this loss function, this point, this point, you see this point, which means a line, right? Or a linear model. Has the same loss function, loss value like this point. How did I know? They are on the same contour. Although they look far from each other, right? They are two different models. But they have the same loss value, which means they have the same quality. Goodness. According to this loss function. Okay. These are different lines, of course. You mean linear models or you mean the contours? Yeah, the two dots are on the same contour. Maybe you didn't, yeah, I think this was the loss function. I think this was the line, the point, sorry. Yeah, so they are on the same contour. Okay. Yes. That's what I meant. Yeah. Okay. So shades. The brightness means the loss is increasing. Darkness means loss is decreasing. And these contours gives you, gives us the points where the loss is the same. Okay. How can we find a good model? Usually this is an iterative process. We start with one. This point, let's say this point. And we hope to move towards minimizing the loss function. Okay. Now, if you remember from calculus, when we compute the gradient or the, if we go, if we go through the line, through the direction of the slope of the function, we are increasing the function or decreasing the function? We are increasing the output of the function or we are decreasing the output of the function? If we are going through the slope, in the direction of the slope of the function. Okay. Let's say that we are talking about this line. Okay. Yeah, so the slope is positive, right? So if we go through the direction of the slope, which is the positive direction, we are increasing the value of the function, right? Right. This is if, if we're talking about this function, but we are talking about the loss function. Okay. So if we go through the slope of the loss function. Okay. What do you mean by going through the slope of the loss function? We are changing the values of the parameters in the direction of the slope of the loss function. Then we are increasing the value of the loss, right? Because we are going through the slope. Is that what we want? No, we want to decrease, right? So we usually go through the reverse direction of the slope. Step by step. Okay. So going from this point, the slope of the loss function is this, is this way, because we are increasing the loss function in this way. So we go in the reverse direction. We go this way. Okay. And then again, repeat again and go to another point. So whenever we take steps like that, we are changing the values of the parameters to find better models. How do we know that that might be a better model? If the loss is decreasing. Right. So we continue in iterations until we find a reasonable one. This process is done in the training phase. Okay. We'll talk about details of this later, inshallah. But that's the idea. We try to minimize to find a global minimum of the loss function. So we go in the reverse direction of the slope, which is the gradient. Okay. With respect to the parameters. We'll talk in detail about that, inshallah, later. And as we move, what are we changing? From point zero to point one, what are we changing? The parameters, which means the line, the linear model. Okay. So if you look on the right side, you will see that this is point zero. This is point one. And because we are going, seems like we're going in the right direction, right? Right direction of minimizing the loss. We are actually moving closer to the training points. Okay. So to point two, to point three, point four, it seems like we are getting better models over time. This is very simple example and very simple problem. Usually it's not that easy to find the global minimum. Again, this is the mathematical form. We try to find the minimum values of the parameters, phi hat. I mean, not the minimum values of the parameters. The values of the parameters that make the loss function minimum. There is a difference, by the way. Okay. It's not about minimizing the parameters. We are not minimizing the values of the parameters. We are trying to find the values of the parameters, whatever they are, that minimizes the loss function. We want to minimize the loss. We want to minimize the error. Okay. So we are trying to find the parameters that will do that. When we find the parameters, it's like finding a line, right? Because the line is specified by the two parameters. In general, any model is specified by a set of parameters. So in general, in training, any model, not just the linear regression model, we are trying to find the parameters that minimize the loss function. A value for each parameter. But in reality, how many parameters we have in the model? In general, there are so many. Let me give you just one example. You know charge-gbt? How many parameters we have? Any idea? Billions of parameters. Hundreds of billions of parameters. Not just two. So here we manage to look at the loss function, right? Because we have just two parameters and just the height is 3D. If it's just three, can you see it? If we have three parameters, can we visualize it in 4D? Can we think of 4D space? No. Imagine billions, not just three. It's very, very complex. But the idea is the same. We are trying to find the values of the parameters that will minimize the loss function. Did you understand now why we started with this linear regression? Because that's maybe among a few that we can actually visualize. And it will, inshallah, act as a very good basis for the neural networks that we will discuss, inshallah. Hopefully, maybe we can start today or next time. Now we talked about this form, right, of the loss function. It's the model minus the predicted value minus the actual value. The predicted value, if we expand the function, is just phi 0 plus phi 1 times Xi. That's the predicted value from the line equation. Now the process that we just discussed is called, or the procedure that we just discussed, of trying to find the global minimum is called gradient descent. And we'll talk about it, inshallah, again in detail later. It's an iterative process to find a good node in the array. Now, if you remember from linear algebra, we can find the minimum of a function, right? I think you learned how to find minimum of a function, right? We just get the derivative and we equal it to 0. We solve for the parameters, right? And for this specific example, yes, you can do that. You can get a closed formula. You can get a closed formula. Okay? So why don't we get a closed formula and don't do this iterative process? We don't need to try multiple lines or to try multiple values or parameters to get the best of them. We can actually compute it. Mathematically, we can find the minimum of this function. Done. Why should we use gradient descent? Yes. So in this simple case, there is a closed formula, okay? Because we have two parameters. The function is convex. It's really easy. But in general, in more complex problems, that's not possible. Okay? So it's not possible to get this closed form. So we have to do it. Try and error. Okay. Why don't we just try every possible line or every possible model? We will go iteratively, right? We are trying one model and then we change it, try another model and so on, right? Why don't we just try all of them and get the best? So with two parameters, we have so many, right? With billions of parameters? Okay. So we will not be able to do that. So that means that the training, the algorithm or the procedure to find the best parameters has to be very efficient because it searches in a huge space of parameters. Billions of parameters and each parameter probably is a real number. So it has infinite possible values. So it's unimaginable. Unless we do it in an efficient way and effective way. And do it quickly and do it in a way that will really give us something that is reasonable. Quick poll. Fifth thing about it is to find values or parameters for good predictions in and then dots. Okay? Is it in training or in testing or in both training and testing? So fitting a model is to find values or parameters for good predictions in. Is it training only, testing only or both training and testing? Fitting a model. We are fitting a model now. So do we get the, do we find the values or parameters based on the test data? So that's only for training, in training. We don't use the predictions of tests to guide the fitting. Okay? That's, we don't call it test set. We talk about the test set. That's, we don't call it test set. We'll talk about it later. That's called div set. That's a different story. But for test set, it's completely unseen. Test set means you cannot see or use in training and fitting the model. Ever. Okay? Here's another one. In training a model, now it's very clear. In training a model, we see parameters that. Maximize or minimize the loss function on training or test? So minimize or maximize? Minimize. Minimize. The loss function on? On training data. Okay. That's easy, right? Okay. Okay. So now, in testing, as we said earlier, we have to test on separate unseen data. So we have to test on separate unseen set of input-output data. Okay? So we will compute the predictions on the unseen input values and compare it and use the loss and we can compute either the loss or we can compute a performance measure. We'll talk about performance measures in a little bit to tell us whether it's a good or bad model. And the degree to which this is same as training is indicating generalization. So we will measure, as we saw, we measure the performance using the loss function while we are doing the training, right? We'll do something similar on test. Okay. What if both are good? I mean, when we compute the loss on the training, it was really good. And when we computed the loss on the or the performance on the test set, it was also good, as good as the training. So does that indicate somehow the generalization? Yes, if the choice of the test set is good. Okay? Yes. But that's not always the case. Sometimes we get what we call underfitting. The quality or the performance on the training set is bad and on the test set is bad. Why could that be the problem related to what you said? Okay, sometimes we try to fit a model that is very simple to a very complex problem, very complex data. Okay? So let's say that the data was a curve like that. Okay? But we try to represent it as a line. If you try infinite lines, it will be bad. Okay? That's called underfitting. The model is too simple. Okay? Time. In the other extreme, maybe the model might be too complex. So when you try to fit it to the data, it was fit even to the noise in the data. Sometimes data has noise. Okay? And errors in measurement. For example, if I, instead of saying the age as a real number, like 14.6, I tell you 15. Or I measure it 15. Okay? And that's noise. That's noise based on how we collected the data. The noise also can be in the output, not only in the input. Okay? So if the model is too complex, it will give attention to every detail in the data. So it will fit even to the noise. Now, when we try it on the test set, so when we do it on the training, it will be perfect. You might even get zero loss because it fits the data very well. Okay? But that will not generalize well. So if you test it on the unseen data, it will not be as good as the train. We call this overfitting. Okay? So you see the difference? Let me show you this example. I think it will be even more clear. This is again like the data that looks like quadratic. Okay? This one. But we try to fit it with a line. It's a simple model. This model is not quadratic anyway. It's linear. Okay? And it tries to fit to data that is, to a relationship that is quadratic. Whenever you will do, it will be bad. So this is underfitting. Why is it underfitting? Not because the look of it, but because it has very bad training data, training error, and also on the test it will be bad. Look at the other way. This is same data, by the way. The points are the same over the three plots here. Look now at that model. This is not a linear model and not a quadratic model. This is polynomial model that's very complex. So as you see here, it tries to fit the data very well. Although the best actual model that represents this data is just this. So this one will give us very good error value, right? Very minimal in on training. But in test, it might not be good. So it doesn't generalize. It actually fits even to the noise in this. So this is an example of overfitting, and this is balanced. Probably this is the one that will generalize better to unseal it. You mean the performance. Yes, there will be a performance function that we will use on test, and there is also the loss function that we use on training. For now, think of a way to measure the performance, how good it is. So for underfitting, it will be bad for both training and test, because it doesn't fit as well. It's very simple. For overfitting, yes, the testing will be much worse than the performance. On testing, it will be much worse than training. So that's an indication of overfitting. Okay, quick question. A model that has zero loss on training data, think about this, please, before you answer again and again and again. A model that has zero loss on training data is the best possible model we can get, is not guaranteed to be good in reality, indicates overfitting, indicates underfitting. So we might have zero answer, might have one answer, might have more than one answer. Think of it, and you know what's the next question. You know what's the next question for any answer. You know? When you give me an answer, what's my next question? Why? So you have to be prepared for that. Yes? It indicates? Can we? Okay, so let's take one by one. So you said two answers, right? It indicates overfitting and? Okay, let's take one by one. Indicates overfitting. Can we say that it's overfitting without this set? The statement says that the model has zero loss on training data. Does it say anything about this? No, so we cannot say overfitting, right? We cannot say overfitting. Having zero loss on training set doesn't mean always overfitting, by the way. Maybe it would be good also on this set. Okay. So the model is balanced and it fits the data well on test, on training. It's not overfitting. Okay, and you are trying to change the question now. Okay, let's continue the answer and then we get back to you. So, yes, so your next answer is not guaranteed to be good in reality. Why? Yeah, we have to test it on unseen data. Okay? It doesn't mean if we got zero error on training data, that doesn't mean that it's the best we got. That's the first answer. Of course not. It's not guaranteed to be good in reality. Maybe it's overfitting. Maybe it's overfitting. But it doesn't also indicate overfitting. We cannot say for sure that it indicates overfitting. And of course it's not underfitting. Underfitting is really bad on training. Okay, so why not the first answer? Why not the first answer? It's the best possible model we can get? No, it might not generalize, right? Yeah, it's, yeah. But with the information that we have, we cannot say it's the best. By the way, if we have good error on, I mean, good performance on training and good performance on test, does that mean that it will work very well in reality? It depends on how you... It's not also guaranteed because maybe the test set is not chosen well. Maybe the test set was chosen to be easy examples. It's like an exam. You were really good. You got A in the course. Okay? Does that mean that in reality you are an A student? Maybe the exam was really easy. It didn't actually examine you well or indicate how good you understood the material. Okay? Hopefully it's good, but it depends on how you were tested on unsealed data. Okay? Any other questions? Okay, we'll stop here, inshallah. Next time we will talk about, we'll start to talk about neural networks. Okay? Thank you very much and see you, inshallah, on Wednesday.